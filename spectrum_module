digraph {
	graph [size="104.55,104.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1649475155584 [label="
 ()" fillcolor=darkolivegreen1]
	1649474952256 [label=MeanBackward0]
	1649474952928 -> 1649474952256
	1649474952928 [label=AddmmBackward0]
	1649474952784 -> 1649474952928
	1649475101136 [label="out_layer.3.bias
 (32)" fillcolor=lightblue]
	1649475101136 -> 1649474952784
	1649474952784 [label=AccumulateGrad]
	1649474953120 -> 1649474952928
	1649474953120 [label=MulBackward0]
	1649474952736 -> 1649474953120
	1649474952736 [label=ReluBackward0]
	1649475141792 -> 1649474952736
	1649475141792 [label=AddmmBackward0]
	1649475142032 -> 1649475141792
	1649475100976 [label="out_layer.0.bias
 (32)" fillcolor=lightblue]
	1649475100976 -> 1649475142032
	1649475142032 [label=AccumulateGrad]
	1649475141840 -> 1649475141792
	1649475141840 [label=ViewBackward0]
	1649475142128 -> 1649475141840
	1649475142128 [label=CloneBackward0]
	1649475142368 -> 1649475142128
	1649475142368 [label=TransposeBackward0]
	1649475142464 -> 1649475142368
	1649475142464 [label=ViewBackward0]
	1649475142560 -> 1649475142464
	1649475142560 [label=ViewBackward0]
	1649475142656 -> 1649475142560
	1649475142656 [label=AddmmBackward0]
	1649475142752 -> 1649475142656
	1649475100816 [label="attention.out.bias
 (32)" fillcolor=lightblue]
	1649475100816 -> 1649475142752
	1649475142752 [label=AccumulateGrad]
	1649475142704 -> 1649475142656
	1649475142704 [label=ViewBackward0]
	1649475142848 -> 1649475142704
	1649475142848 [label=ViewBackward0]
	1649475143040 -> 1649475142848
	1649475143040 [label=CloneBackward0]
	1649475143136 -> 1649475143040
	1649475143136 [label=TransposeBackward0]
	1649475143232 -> 1649475143136
	1649475143232 [label=UnsafeViewBackward0]
	1649475143328 -> 1649475143232
	1649475143328 [label=BmmBackward0]
	1649475143424 -> 1649475143328
	1649475143424 [label=ViewBackward0]
	1649475143568 -> 1649475143424
	1649475143568 [label=ExpandBackward0]
	1649475143664 -> 1649475143568
	1649475143664 [label=SoftmaxBackward0]
	1649475143712 -> 1649475143664
	1649475143712 [label=DivBackward0]
	1649475143856 -> 1649475143712
	1649475143856 [label=UnsafeViewBackward0]
	1649475144048 -> 1649475143856
	1649475144048 [label=BmmBackward0]
	1649475144096 -> 1649475144048
	1649475144096 [label=UnsafeViewBackward0]
	1649475144336 -> 1649475144096
	1649475144336 [label=CloneBackward0]
	1649475144384 -> 1649475144336
	1649475144384 [label=ExpandBackward0]
	1649475144528 -> 1649475144384
	1649475144528 [label=TransposeBackward0]
	1649475144672 -> 1649475144528
	1649475144672 [label=ViewBackward0]
	1649475144816 -> 1649475144672
	1649475144816 [label=ViewBackward0]
	1649475144960 -> 1649475144816
	1649475144960 [label=AddmmBackward0]
	1649475145104 -> 1649475144960
	1649474907728 [label="attention.query.bias
 (32)" fillcolor=lightblue]
	1649474907728 -> 1649475145104
	1649475145104 [label=AccumulateGrad]
	1649475145056 -> 1649475144960
	1649475145056 [label=ViewBackward0]
	1649475145296 -> 1649475145056
	1649475145296 [label=UnsafeViewBackward0]
	1649475145584 -> 1649475145296
	1649475145584 [label=CloneBackward0]
	1649475145680 -> 1649475145584
	1649475145680 [label=TransposeBackward0]
	1649475186800 -> 1649475145680
	1649475186800 [label=ReluBackward0]
	1649475186896 -> 1649475186800
	1649475186896 [label=AddBackward0]
	1649475187040 -> 1649475186896
	1649475187040 [label=MulBackward0]
	1649475187280 -> 1649475187040
	1649475187280 [label=NativeBatchNormBackward0]
	1649475187424 -> 1649475187280
	1649475187424 [label=ConvolutionBackward0]
	1649475187616 -> 1649475187424
	1649475187616 [label=MulBackward0]
	1649475187808 -> 1649475187616
	1649475187808 [label=ReluBackward0]
	1649475187904 -> 1649475187808
	1649475187904 [label=NativeBatchNormBackward0]
	1649475187952 -> 1649475187904
	1649475187952 [label=ConvolutionBackward0]
	1649475186992 -> 1649475187952
	1649475186992 [label=ReluBackward0]
	1649475188384 -> 1649475186992
	1649475188384 [label=AddBackward0]
	1649475188432 -> 1649475188384
	1649475188432 [label=MulBackward0]
	1649475188672 -> 1649475188432
	1649475188672 [label=NativeBatchNormBackward0]
	1649475188816 -> 1649475188672
	1649475188816 [label=ConvolutionBackward0]
	1649475189008 -> 1649475188816
	1649475189008 [label=MulBackward0]
	1649475189200 -> 1649475189008
	1649475189200 [label=ReluBackward0]
	1649475189296 -> 1649475189200
	1649475189296 [label=NativeBatchNormBackward0]
	1649475189344 -> 1649475189296
	1649475189344 [label=ConvolutionBackward0]
	1649475188288 -> 1649475189344
	1649475188288 [label=ReluBackward0]
	1649475189776 -> 1649475188288
	1649475189776 [label=AddBackward0]
	1649475189824 -> 1649475189776
	1649475189824 [label=MulBackward0]
	1649475190064 -> 1649475189824
	1649475190064 [label=NativeBatchNormBackward0]
	1649475190208 -> 1649475190064
	1649475190208 [label=ConvolutionBackward0]
	1649475190400 -> 1649475190208
	1649475190400 [label=MulBackward0]
	1649475190592 -> 1649475190400
	1649475190592 [label=ReluBackward0]
	1649475190688 -> 1649475190592
	1649475190688 [label=NativeBatchNormBackward0]
	1649475190496 -> 1649475190688
	1649475190496 [label=ConvolutionBackward0]
	1649475189680 -> 1649475190496
	1649475189680 [label=ReluBackward0]
	1649475203520 -> 1649475189680
	1649475203520 [label=AddBackward0]
	1649475203568 -> 1649475203520
	1649475203568 [label=MulBackward0]
	1649475203808 -> 1649475203568
	1649475203808 [label=NativeBatchNormBackward0]
	1649475203952 -> 1649475203808
	1649475203952 [label=ConvolutionBackward0]
	1649475204144 -> 1649475203952
	1649475204144 [label=MulBackward0]
	1649475204336 -> 1649475204144
	1649475204336 [label=ReluBackward0]
	1649475204432 -> 1649475204336
	1649475204432 [label=NativeBatchNormBackward0]
	1649475204480 -> 1649475204432
	1649475204480 [label=ConvolutionBackward0]
	1649475203424 -> 1649475204480
	1649475203424 [label=ReluBackward0]
	1649475204912 -> 1649475203424
	1649475204912 [label=AddBackward0]
	1649475204960 -> 1649475204912
	1649475204960 [label=MulBackward0]
	1649475205200 -> 1649475204960
	1649475205200 [label=NativeBatchNormBackward0]
	1649475205344 -> 1649475205200
	1649475205344 [label=ConvolutionBackward0]
	1649475205536 -> 1649475205344
	1649475205536 [label=MulBackward0]
	1649475205728 -> 1649475205536
	1649475205728 [label=ReluBackward0]
	1649475205824 -> 1649475205728
	1649475205824 [label=NativeBatchNormBackward0]
	1649475205872 -> 1649475205824
	1649475205872 [label=ConvolutionBackward0]
	1649475204816 -> 1649475205872
	1649475204816 [label=ReluBackward0]
	1649475206304 -> 1649475204816
	1649475206304 [label=AddBackward0]
	1649475206352 -> 1649475206304
	1649475206352 [label=MulBackward0]
	1649475206592 -> 1649475206352
	1649475206592 [label=NativeBatchNormBackward0]
	1649475206736 -> 1649475206592
	1649475206736 [label=ConvolutionBackward0]
	1649475206928 -> 1649475206736
	1649475206928 [label=MulBackward0]
	1649475207120 -> 1649475206928
	1649475207120 [label=ReluBackward0]
	1649475207024 -> 1649475207120
	1649475207024 [label=NativeBatchNormBackward0]
	1649475215520 -> 1649475207024
	1649475215520 [label=ConvolutionBackward0]
	1649475206208 -> 1649475215520
	1649475206208 [label=ViewBackward0]
	1649475215952 -> 1649475206208
	1649475215952 [label=MulBackward0]
	1649475216000 -> 1649475215952
	1649475216000 [label=ReluBackward0]
	1649475216192 -> 1649475216000
	1649475216192 [label=NativeLayerNormBackward0]
	1649475216240 -> 1649475216192
	1649475216240 [label=AddmmBackward0]
	1649475216528 -> 1649475216240
	1649306950688 [label="in_layer.0.bias
 (64)" fillcolor=lightblue]
	1649306950688 -> 1649475216528
	1649475216528 [label=AccumulateGrad]
	1649475216480 -> 1649475216240
	1649475216480 [label=TBackward0]
	1649475216576 -> 1649475216480
	1649450908304 [label="in_layer.0.weight
 (64, 32)" fillcolor=lightblue]
	1649450908304 -> 1649475216576
	1649475216576 [label=AccumulateGrad]
	1649475216096 -> 1649475216192
	1651404582800 [label="in_layer.1.weight
 (64)" fillcolor=lightblue]
	1651404582800 -> 1649475216096
	1649475216096 [label=AccumulateGrad]
	1649475216336 -> 1649475216192
	1651404582240 [label="in_layer.1.bias
 (64)" fillcolor=lightblue]
	1651404582240 -> 1649475216336
	1649475216336 [label=AccumulateGrad]
	1649475215808 -> 1649475215520
	1649474647680 [label="blocks.0.conv1.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474647680 -> 1649475215808
	1649475215808 [label=AccumulateGrad]
	1649475215760 -> 1649475215520
	1649474647760 [label="blocks.0.conv1.bias
 (2)" fillcolor=lightblue]
	1649474647760 -> 1649475215760
	1649475215760 [label=AccumulateGrad]
	1649475215472 -> 1649475207024
	1649474647840 [label="blocks.0.bn1.weight
 (2)" fillcolor=lightblue]
	1649474647840 -> 1649475215472
	1649475215472 [label=AccumulateGrad]
	1649475215616 -> 1649475207024
	1649401413440 [label="blocks.0.bn1.bias
 (2)" fillcolor=lightblue]
	1649401413440 -> 1649475215616
	1649475215616 [label=AccumulateGrad]
	1649475206880 -> 1649475206736
	1649474648320 [label="blocks.0.conv2.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474648320 -> 1649475206880
	1649475206880 [label=AccumulateGrad]
	1649475206832 -> 1649475206736
	1649474648400 [label="blocks.0.conv2.bias
 (2)" fillcolor=lightblue]
	1649474648400 -> 1649475206832
	1649475206832 [label=AccumulateGrad]
	1649475206688 -> 1649475206592
	1649474648480 [label="blocks.0.bn2.weight
 (2)" fillcolor=lightblue]
	1649474648480 -> 1649475206688
	1649475206688 [label=AccumulateGrad]
	1649475206640 -> 1649475206592
	1649474648560 [label="blocks.0.bn2.bias
 (2)" fillcolor=lightblue]
	1649474648560 -> 1649475206640
	1649475206640 [label=AccumulateGrad]
	1649475206544 -> 1649475206352
	1649475206544 [label=ExpandBackward0]
	1649475207072 -> 1649475206544
	1649475207072 [label=ViewBackward0]
	1649475206784 -> 1649475207072
	1649475206784 [label=SigmoidBackward0]
	1649475215904 -> 1649475206784
	1649475215904 [label=MmBackward0]
	1649475216144 -> 1649475215904
	1649475216144 [label=ReluBackward0]
	1649475216720 -> 1649475216144
	1649475216720 [label=MmBackward0]
	1649475216864 -> 1649475216720
	1649475216864 [label=ViewBackward0]
	1649475217008 -> 1649475216864
	1649475217008 [label=SqueezeBackward1]
	1649475217104 -> 1649475217008
	1649475217104 [label=MeanBackward1]
	1649475217200 -> 1649475217104
	1649475217200 [label=UnsqueezeBackward0]
	1649475206592 -> 1649475217200
	1649475216912 -> 1649475216720
	1649475216912 [label=TBackward0]
	1649475217152 -> 1649475216912
	1649474648800 [label="blocks.0.se.fc.0.weight
 (0, 2)" fillcolor=lightblue]
	1649474648800 -> 1649475217152
	1649475217152 [label=AccumulateGrad]
	1649475215856 -> 1649475215904
	1649475215856 [label=TBackward0]
	1649475217056 -> 1649475215856
	1649474648880 [label="blocks.0.se.fc.2.weight
 (2, 0)" fillcolor=lightblue]
	1649474648880 -> 1649475217056
	1649475217056 [label=AccumulateGrad]
	1649475206208 -> 1649475206304
	1649475206160 -> 1649475205872
	1649474649040 [label="blocks.1.conv1.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474649040 -> 1649475206160
	1649475206160 [label=AccumulateGrad]
	1649475206112 -> 1649475205872
	1649474741568 [label="blocks.1.conv1.bias
 (2)" fillcolor=lightblue]
	1649474741568 -> 1649475206112
	1649475206112 [label=AccumulateGrad]
	1649475205632 -> 1649475205824
	1649474741648 [label="blocks.1.bn1.weight
 (2)" fillcolor=lightblue]
	1649474741648 -> 1649475205632
	1649475205632 [label=AccumulateGrad]
	1649475205968 -> 1649475205824
	1649474741728 [label="blocks.1.bn1.bias
 (2)" fillcolor=lightblue]
	1649474741728 -> 1649475205968
	1649475205968 [label=AccumulateGrad]
	1649475205488 -> 1649475205344
	1649474742048 [label="blocks.1.conv2.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474742048 -> 1649475205488
	1649475205488 [label=AccumulateGrad]
	1649475205440 -> 1649475205344
	1649474742128 [label="blocks.1.conv2.bias
 (2)" fillcolor=lightblue]
	1649474742128 -> 1649475205440
	1649475205440 [label=AccumulateGrad]
	1649475205296 -> 1649475205200
	1649474742208 [label="blocks.1.bn2.weight
 (2)" fillcolor=lightblue]
	1649474742208 -> 1649475205296
	1649475205296 [label=AccumulateGrad]
	1649475205248 -> 1649475205200
	1649474742288 [label="blocks.1.bn2.bias
 (2)" fillcolor=lightblue]
	1649474742288 -> 1649475205248
	1649475205248 [label=AccumulateGrad]
	1649475205152 -> 1649475204960
	1649475205152 [label=ExpandBackward0]
	1649475205680 -> 1649475205152
	1649475205680 [label=ViewBackward0]
	1649475206016 -> 1649475205680
	1649475206016 [label=SigmoidBackward0]
	1649475206256 -> 1649475206016
	1649475206256 [label=MmBackward0]
	1649475206496 -> 1649475206256
	1649475206496 [label=ReluBackward0]
	1649475206976 -> 1649475206496
	1649475206976 [label=MmBackward0]
	1649475216384 -> 1649475206976
	1649475216384 [label=ViewBackward0]
	1649475216432 -> 1649475216384
	1649475216432 [label=SqueezeBackward1]
	1649475217344 -> 1649475216432
	1649475217344 [label=MeanBackward1]
	1649475217440 -> 1649475217344
	1649475217440 [label=UnsqueezeBackward0]
	1649475205200 -> 1649475217440
	1649475215424 -> 1649475206976
	1649475215424 [label=TBackward0]
	1649475217392 -> 1649475215424
	1649474742608 [label="blocks.1.se.fc.0.weight
 (0, 2)" fillcolor=lightblue]
	1649474742608 -> 1649475217392
	1649475217392 [label=AccumulateGrad]
	1649475206448 -> 1649475206256
	1649475206448 [label=TBackward0]
	1649475216960 -> 1649475206448
	1649474742688 [label="blocks.1.se.fc.2.weight
 (2, 0)" fillcolor=lightblue]
	1649474742688 -> 1649475216960
	1649475216960 [label=AccumulateGrad]
	1649475204816 -> 1649475204912
	1649475204768 -> 1649475204480
	1649474742768 [label="blocks.2.conv1.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474742768 -> 1649475204768
	1649475204768 [label=AccumulateGrad]
	1649475204720 -> 1649475204480
	1649474742848 [label="blocks.2.conv1.bias
 (2)" fillcolor=lightblue]
	1649474742848 -> 1649475204720
	1649475204720 [label=AccumulateGrad]
	1649475204240 -> 1649475204432
	1649474742928 [label="blocks.2.bn1.weight
 (2)" fillcolor=lightblue]
	1649474742928 -> 1649475204240
	1649475204240 [label=AccumulateGrad]
	1649475204576 -> 1649475204432
	1649474743008 [label="blocks.2.bn1.bias
 (2)" fillcolor=lightblue]
	1649474743008 -> 1649475204576
	1649475204576 [label=AccumulateGrad]
	1649475204096 -> 1649475203952
	1649474743328 [label="blocks.2.conv2.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474743328 -> 1649475204096
	1649475204096 [label=AccumulateGrad]
	1649475204048 -> 1649475203952
	1649474743408 [label="blocks.2.conv2.bias
 (2)" fillcolor=lightblue]
	1649474743408 -> 1649475204048
	1649475204048 [label=AccumulateGrad]
	1649475203904 -> 1649475203808
	1649474743488 [label="blocks.2.bn2.weight
 (2)" fillcolor=lightblue]
	1649474743488 -> 1649475203904
	1649475203904 [label=AccumulateGrad]
	1649475203856 -> 1649475203808
	1649474743568 [label="blocks.2.bn2.bias
 (2)" fillcolor=lightblue]
	1649474743568 -> 1649475203856
	1649475203856 [label=AccumulateGrad]
	1649475203760 -> 1649475203568
	1649475203760 [label=ExpandBackward0]
	1649475204288 -> 1649475203760
	1649475204288 [label=ViewBackward0]
	1649475204624 -> 1649475204288
	1649475204624 [label=SigmoidBackward0]
	1649475204864 -> 1649475204624
	1649475204864 [label=MmBackward0]
	1649475205104 -> 1649475204864
	1649475205104 [label=ReluBackward0]
	1649475206064 -> 1649475205104
	1649475206064 [label=MmBackward0]
	1649475205392 -> 1649475206064
	1649475205392 [label=ViewBackward0]
	1649475215712 -> 1649475205392
	1649475215712 [label=SqueezeBackward1]
	1649475217248 -> 1649475215712
	1649475217248 [label=MeanBackward1]
	1649475217632 -> 1649475217248
	1649475217632 [label=UnsqueezeBackward0]
	1649475203808 -> 1649475217632
	1649475205584 -> 1649475206064
	1649475205584 [label=TBackward0]
	1649475217584 -> 1649475205584
	1649474743888 [label="blocks.2.se.fc.0.weight
 (0, 2)" fillcolor=lightblue]
	1649474743888 -> 1649475217584
	1649475217584 [label=AccumulateGrad]
	1649475205056 -> 1649475204864
	1649475205056 [label=TBackward0]
	1649475205776 -> 1649475205056
	1649474743968 [label="blocks.2.se.fc.2.weight
 (2, 0)" fillcolor=lightblue]
	1649474743968 -> 1649475205776
	1649475205776 [label=AccumulateGrad]
	1649475203424 -> 1649475203520
	1649475203376 -> 1649475190496
	1649474744048 [label="blocks.3.conv1.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474744048 -> 1649475203376
	1649475203376 [label=AccumulateGrad]
	1649475203328 -> 1649475190496
	1649474744128 [label="blocks.3.conv1.bias
 (2)" fillcolor=lightblue]
	1649474744128 -> 1649475203328
	1649475203328 [label=AccumulateGrad]
	1649475203136 -> 1649475190688
	1649474744208 [label="blocks.3.bn1.weight
 (2)" fillcolor=lightblue]
	1649474744208 -> 1649475203136
	1649475203136 [label=AccumulateGrad]
	1649475203184 -> 1649475190688
	1649474904128 [label="blocks.3.bn1.bias
 (2)" fillcolor=lightblue]
	1649474904128 -> 1649475203184
	1649475203184 [label=AccumulateGrad]
	1649475190352 -> 1649475190208
	1649474904448 [label="blocks.3.conv2.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474904448 -> 1649475190352
	1649475190352 [label=AccumulateGrad]
	1649475190304 -> 1649475190208
	1649474904528 [label="blocks.3.conv2.bias
 (2)" fillcolor=lightblue]
	1649474904528 -> 1649475190304
	1649475190304 [label=AccumulateGrad]
	1649475190160 -> 1649475190064
	1649474904608 [label="blocks.3.bn2.weight
 (2)" fillcolor=lightblue]
	1649474904608 -> 1649475190160
	1649475190160 [label=AccumulateGrad]
	1649475190112 -> 1649475190064
	1649474904688 [label="blocks.3.bn2.bias
 (2)" fillcolor=lightblue]
	1649474904688 -> 1649475190112
	1649475190112 [label=AccumulateGrad]
	1649475190016 -> 1649475189824
	1649475190016 [label=ExpandBackward0]
	1649475190544 -> 1649475190016
	1649475190544 [label=ViewBackward0]
	1649475190640 -> 1649475190544
	1649475190640 [label=SigmoidBackward0]
	1649475203472 -> 1649475190640
	1649475203472 [label=MmBackward0]
	1649475203712 -> 1649475203472
	1649475203712 [label=ReluBackward0]
	1649475204672 -> 1649475203712
	1649475204672 [label=MmBackward0]
	1649475204000 -> 1649475204672
	1649475204000 [label=ViewBackward0]
	1649475215664 -> 1649475204000
	1649475215664 [label=SqueezeBackward1]
	1649475217776 -> 1649475215664
	1649475217776 [label=MeanBackward1]
	1649475217872 -> 1649475217776
	1649475217872 [label=UnsqueezeBackward0]
	1649475190064 -> 1649475217872
	1649475204192 -> 1649475204672
	1649475204192 [label=TBackward0]
	1649475217824 -> 1649475204192
	1649474905008 [label="blocks.3.se.fc.0.weight
 (0, 2)" fillcolor=lightblue]
	1649474905008 -> 1649475217824
	1649475217824 [label=AccumulateGrad]
	1649475203664 -> 1649475203472
	1649475203664 [label=TBackward0]
	1649475204384 -> 1649475203664
	1649474905088 [label="blocks.3.se.fc.2.weight
 (2, 0)" fillcolor=lightblue]
	1649474905088 -> 1649475204384
	1649475204384 [label=AccumulateGrad]
	1649475189680 -> 1649475189776
	1649475189632 -> 1649475189344
	1649474905168 [label="blocks.4.conv1.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474905168 -> 1649475189632
	1649475189632 [label=AccumulateGrad]
	1649475189584 -> 1649475189344
	1649474905248 [label="blocks.4.conv1.bias
 (2)" fillcolor=lightblue]
	1649474905248 -> 1649475189584
	1649475189584 [label=AccumulateGrad]
	1649475189104 -> 1649475189296
	1649474905328 [label="blocks.4.bn1.weight
 (2)" fillcolor=lightblue]
	1649474905328 -> 1649475189104
	1649475189104 [label=AccumulateGrad]
	1649475189440 -> 1649475189296
	1649474905408 [label="blocks.4.bn1.bias
 (2)" fillcolor=lightblue]
	1649474905408 -> 1649475189440
	1649475189440 [label=AccumulateGrad]
	1649475188960 -> 1649475188816
	1649474905728 [label="blocks.4.conv2.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474905728 -> 1649475188960
	1649475188960 [label=AccumulateGrad]
	1649475188912 -> 1649475188816
	1649474905808 [label="blocks.4.conv2.bias
 (2)" fillcolor=lightblue]
	1649474905808 -> 1649475188912
	1649475188912 [label=AccumulateGrad]
	1649475188768 -> 1649475188672
	1649474905888 [label="blocks.4.bn2.weight
 (2)" fillcolor=lightblue]
	1649474905888 -> 1649475188768
	1649475188768 [label=AccumulateGrad]
	1649475188720 -> 1649475188672
	1649474905968 [label="blocks.4.bn2.bias
 (2)" fillcolor=lightblue]
	1649474905968 -> 1649475188720
	1649475188720 [label=AccumulateGrad]
	1649475188624 -> 1649475188432
	1649475188624 [label=ExpandBackward0]
	1649475189152 -> 1649475188624
	1649475189152 [label=ViewBackward0]
	1649475189488 -> 1649475189152
	1649475189488 [label=SigmoidBackward0]
	1649475189728 -> 1649475189488
	1649475189728 [label=MmBackward0]
	1649475189968 -> 1649475189728
	1649475189968 [label=ReluBackward0]
	1649475190256 -> 1649475189968
	1649475190256 [label=MmBackward0]
	1649475203232 -> 1649475190256
	1649475203232 [label=ViewBackward0]
	1649475216768 -> 1649475203232
	1649475216768 [label=SqueezeBackward1]
	1649475217488 -> 1649475216768
	1649475217488 [label=MeanBackward1]
	1649475218064 -> 1649475217488
	1649475218064 [label=UnsqueezeBackward0]
	1649475188672 -> 1649475218064
	1649475203280 -> 1649475190256
	1649475203280 [label=TBackward0]
	1649475218016 -> 1649475203280
	1649474906288 [label="blocks.4.se.fc.0.weight
 (0, 2)" fillcolor=lightblue]
	1649474906288 -> 1649475218016
	1649475218016 [label=AccumulateGrad]
	1649475189920 -> 1649475189728
	1649475189920 [label=TBackward0]
	1649475190448 -> 1649475189920
	1649474906368 [label="blocks.4.se.fc.2.weight
 (2, 0)" fillcolor=lightblue]
	1649474906368 -> 1649475190448
	1649475190448 [label=AccumulateGrad]
	1649475188288 -> 1649475188384
	1649475188240 -> 1649475187952
	1649474906448 [label="blocks.5.conv1.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474906448 -> 1649475188240
	1649475188240 [label=AccumulateGrad]
	1649475188192 -> 1649475187952
	1649474906528 [label="blocks.5.conv1.bias
 (2)" fillcolor=lightblue]
	1649474906528 -> 1649475188192
	1649475188192 [label=AccumulateGrad]
	1649475187712 -> 1649475187904
	1649474906608 [label="blocks.5.bn1.weight
 (2)" fillcolor=lightblue]
	1649474906608 -> 1649475187712
	1649475187712 [label=AccumulateGrad]
	1649475188048 -> 1649475187904
	1649474906688 [label="blocks.5.bn1.bias
 (2)" fillcolor=lightblue]
	1649474906688 -> 1649475188048
	1649475188048 [label=AccumulateGrad]
	1649475187568 -> 1649475187424
	1649474907008 [label="blocks.5.conv2.weight
 (2, 2, 3)" fillcolor=lightblue]
	1649474907008 -> 1649475187568
	1649475187568 [label=AccumulateGrad]
	1649475187520 -> 1649475187424
	1649474907088 [label="blocks.5.conv2.bias
 (2)" fillcolor=lightblue]
	1649474907088 -> 1649475187520
	1649475187520 [label=AccumulateGrad]
	1649475187376 -> 1649475187280
	1649474907168 [label="blocks.5.bn2.weight
 (2)" fillcolor=lightblue]
	1649474907168 -> 1649475187376
	1649475187376 [label=AccumulateGrad]
	1649475187328 -> 1649475187280
	1649474907248 [label="blocks.5.bn2.bias
 (2)" fillcolor=lightblue]
	1649474907248 -> 1649475187328
	1649475187328 [label=AccumulateGrad]
	1649475187232 -> 1649475187040
	1649475187232 [label=ExpandBackward0]
	1649475187760 -> 1649475187232
	1649475187760 [label=ViewBackward0]
	1649475188096 -> 1649475187760
	1649475188096 [label=SigmoidBackward0]
	1649475188336 -> 1649475188096
	1649475188336 [label=MmBackward0]
	1649475188576 -> 1649475188336
	1649475188576 [label=ReluBackward0]
	1649475189536 -> 1649475188576
	1649475189536 [label=MmBackward0]
	1649475188864 -> 1649475189536
	1649475188864 [label=ViewBackward0]
	1649475217536 -> 1649475188864
	1649475217536 [label=SqueezeBackward1]
	1649475217968 -> 1649475217536
	1649475217968 [label=MeanBackward1]
	1649475218256 -> 1649475217968
	1649475218256 [label=UnsqueezeBackward0]
	1649475187280 -> 1649475218256
	1649475189056 -> 1649475189536
	1649475189056 [label=TBackward0]
	1649475218208 -> 1649475189056
	1649474907568 [label="blocks.5.se.fc.0.weight
 (0, 2)" fillcolor=lightblue]
	1649474907568 -> 1649475218208
	1649475218208 [label=AccumulateGrad]
	1649475188528 -> 1649475188336
	1649475188528 [label=TBackward0]
	1649475189248 -> 1649475188528
	1649474907648 [label="blocks.5.se.fc.2.weight
 (2, 0)" fillcolor=lightblue]
	1649474907648 -> 1649475189248
	1649475189248 [label=AccumulateGrad]
	1649475186992 -> 1649475186896
	1649475145200 -> 1649475144960
	1649475145200 [label=TBackward0]
	1649475145632 -> 1649475145200
	1649474907488 [label="attention.query.weight
 (32, 32)" fillcolor=lightblue]
	1649474907488 -> 1649475145632
	1649475145632 [label=AccumulateGrad]
	1649475143952 -> 1649475144048
	1649475143952 [label=UnsafeViewBackward0]
	1649475144480 -> 1649475143952
	1649475144480 [label=CloneBackward0]
	1649475144768 -> 1649475144480
	1649475144768 [label=ExpandBackward0]
	1649475145248 -> 1649475144768
	1649475145248 [label=TransposeBackward0]
	1649475145440 -> 1649475145248
	1649475145440 [label=TransposeBackward0]
	1649475144288 -> 1649475145440
	1649475144288 [label=ViewBackward0]
	1649475187184 -> 1649475144288
	1649475187184 [label=ViewBackward0]
	1649475187856 -> 1649475187184
	1649475187856 [label=AddmmBackward0]
	1649475187472 -> 1649475187856
	1649474907888 [label="attention.key.bias
 (32)" fillcolor=lightblue]
	1649474907888 -> 1649475187472
	1649475187472 [label=AccumulateGrad]
	1649475188144 -> 1649475187856
	1649475188144 [label=ViewBackward0]
	1649475145296 -> 1649475188144
	1649475186848 -> 1649475187856
	1649475186848 [label=TBackward0]
	1649475217680 -> 1649475186848
	1649474907808 [label="attention.key.weight
 (32, 32)" fillcolor=lightblue]
	1649474907808 -> 1649475217680
	1649475217680 [label=AccumulateGrad]
	1649475143376 -> 1649475143328
	1649475143376 [label=UnsafeViewBackward0]
	1649475143472 -> 1649475143376
	1649475143472 [label=CloneBackward0]
	1649475144000 -> 1649475143472
	1649475144000 [label=ExpandBackward0]
	1649475144240 -> 1649475144000
	1649475144240 [label=TransposeBackward0]
	1649475144912 -> 1649475144240
	1649475144912 [label=ViewBackward0]
	1649475145536 -> 1649475144912
	1649475145536 [label=ViewBackward0]
	1649475187664 -> 1649475145536
	1649475187664 [label=AddmmBackward0]
	1649475186752 -> 1649475187664
	1649474908048 [label="attention.value.bias
 (32)" fillcolor=lightblue]
	1649474908048 -> 1649475186752
	1649475186752 [label=AccumulateGrad]
	1649475218304 -> 1649475187664
	1649475218304 [label=ViewBackward0]
	1649475145296 -> 1649475218304
	1649475217920 -> 1649475187664
	1649475217920 [label=TBackward0]
	1649475218400 -> 1649475217920
	1649474907968 [label="attention.value.weight
 (32, 32)" fillcolor=lightblue]
	1649474907968 -> 1649475218400
	1649475218400 [label=AccumulateGrad]
	1649475142224 -> 1649475142656
	1649475142224 [label=TBackward0]
	1649475143088 -> 1649475142224
	1649475100736 [label="attention.out.weight
 (32, 32)" fillcolor=lightblue]
	1649475100736 -> 1649475143088
	1649475143088 [label=AccumulateGrad]
	1649475141696 -> 1649475141792
	1649475141696 [label=TBackward0]
	1649475142416 -> 1649475141696
	1649475100896 [label="out_layer.0.weight
 (32, 64)" fillcolor=lightblue]
	1649475100896 -> 1649475142416
	1649475142416 [label=AccumulateGrad]
	1649474952880 -> 1649474952928
	1649474952880 [label=TBackward0]
	1649474953168 -> 1649474952880
	1649475101056 [label="out_layer.3.weight
 (32, 32)" fillcolor=lightblue]
	1649475101056 -> 1649474953168
	1649474953168 [label=AccumulateGrad]
	1649474952256 -> 1649475155584
}
